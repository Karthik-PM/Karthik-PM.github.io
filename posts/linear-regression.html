<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Understanding Linear Regression — Karthik's Blog</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem;
            background: #f8f9fa;
            color: #212529;
            line-height: 1.6;
        }
        .container {
            background: #fff;
            padding: 2rem;
            border-radius: 10px;
            box-shadow: 0 2px 6px rgba(0,0,0,0.06);
        }
        h1 { color: #1a73e8; margin-bottom: 1rem; }
        .meta { 
            color: #666; 
            margin-bottom: 1.5rem;
            font-size: 0.95rem;
        }
        pre {
            background: #f4f6fb;
            padding: 1rem;
            border-radius: 6px;
            overflow: auto;
        }
        code {
            font-family: 'Consolas', 'Monaco', monospace;
        }
        .math-block {
            background: #f8f9fa;
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
            overflow-x: auto;
        }
        .nav {
            display: flex;
            gap: 1rem;
            margin-bottom: 1.5rem;
        }
        .nav a {
            color: #1a73e8;
            text-decoration: none;
            padding: 0.35rem 0.6rem;
            border-radius: 6px;
            border: 1px solid transparent;
        }
        .nav a:hover {
            background: #e8f0fe;
            border-color: #cfe0fd;
        }
        .back {
            display: inline-block;
            margin-top: 1.5rem;
            color: #1a73e8;
            text-decoration: none;
        }
        .series-nav {
            margin-top: 2rem;
            padding-top: 1rem;
            border-top: 1px solid #eee;
        }
        .series-nav p {
            color: #666;
            margin-bottom: 0.5rem;
        }
    </style>
</head>
<body>
    <div class="container">
        <nav class="nav">
            <a href="/index.html">Home</a>
            <a href="/blog.html">Blog</a>
        </nav>

        <article>
            <h1>Understanding Linear Regression: From Theory to Implementation</h1>
            <p class="meta">Part 1 of the Deep Learning Series — November 8, 2025</p>

            <h2>Introduction</h2>
            <p>Linear regression is one of the fundamental algorithms in machine learning. It serves as a building block for understanding more complex neural networks and deep learning concepts.</p>

            <h2>Mathematical Formulation</h2>
            <div class="math-block">
                <p>Given input features x and target values y, we want to find parameters w (weights) and b (bias) such that:</p>
                <p>ŷ = wx + b</p>
                <p>The loss function (Mean Squared Error) is:</p>
                <p>L(w,b) = 1/n ∑(y - ŷ)²</p>
            </div>

            <h2>Gradient Descent</h2>
            <p>To find the optimal parameters, we use gradient descent. The update rules are:</p>
            <div class="math-block">
                <p>w = w - α * ∂L/∂w</p>
                <p>b = b - α * ∂L/∂b</p>
                <p>where α is the learning rate</p>
            </div>

            <h2>Python Implementation</h2>
            <pre><code>import numpy as np

class LinearRegression:
    def __init__(self, learning_rate=0.01):
        self.lr = learning_rate
        self.w = None
        self.b = None

    def fit(self, X, y, epochs=1000):
        # Initialize parameters
        self.w = np.random.randn(X.shape[1])
        self.b = 0
        
        for _ in range(epochs):
            # Forward pass
            y_pred = np.dot(X, self.w) + self.b
            
            # Compute gradients
            dw = (1/X.shape[0]) * np.dot(X.T, (y_pred - y))
            db = (1/X.shape[0]) * np.sum(y_pred - y)
            
            # Update parameters
            self.w -= self.lr * dw
            self.b -= self.lr * db</code></pre>

            <h2>Key Concepts</h2>
            <ul>
                <li>Linear regression finds a linear relationship between input features and target values</li>
                <li>The loss function measures how well our predictions match the actual values</li>
                <li>Gradient descent iteratively updates parameters to minimize the loss</li>
                <li>The learning rate controls how large our parameter updates are</li>
            </ul>

            <div class="series-nav">
                <p><strong>Deep Learning Series</strong></p>
                <p>Next post coming soon: Neural Networks Basics</p>
            </div>
        </article>

        <a class="back" href="/blog.html">← Back to blog</a>
    </div>
</body>
</html>